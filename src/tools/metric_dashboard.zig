const std = @import("std");
const grove = @import("grove");

/// Nightly metric dashboard generator
/// Collects and publishes Grove performance and health metrics
pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    std.debug.print("=== Grove Metric Dashboard ===\n\n", .{});

    // Parse command line arguments
    const args = try std.process.argsAlloc(allocator);
    defer std.process.argsFree(allocator, args);

    if (args.len < 2) {
        printHelp(args[0]);
        return;
    }

    const command = args[1];

    if (std.mem.eql(u8, command, "generate")) {
        try generateDashboard(allocator);
    } else if (std.mem.eql(u8, command, "health")) {
        try generateHealthReport(allocator);
    } else if (std.mem.eql(u8, command, "performance")) {
        try generatePerformanceReport(allocator);
    } else if (std.mem.eql(u8, command, "serve")) {
        const port = if (args.len >= 3) try std.fmt.parseInt(u16, args[2], 10) else 8080;
        try serveStaticDashboard(allocator, port);
    } else {
        std.debug.print("‚ùå Unknown command: {s}\n", .{command});
        printHelp(args[0]);
    }
}

fn printHelp(program_name: []const u8) void {
    std.debug.print("Usage: {s} <command> [options]\n\n", .{program_name});
    std.debug.print("Commands:\n", .{});
    std.debug.print("  generate      Generate complete dashboard\n", .{});
    std.debug.print("  health        Generate health report only\n", .{});
    std.debug.print("  performance   Generate performance report only\n", .{});
    std.debug.print("  serve [port]  Serve dashboard on HTTP (default port: 8080)\n", .{});
}

const MetricData = struct {
    timestamp: i64,
    version: []const u8,
    health: HealthMetrics,
    performance: PerformanceMetrics,
    build: BuildMetrics,
    languages: LanguageMetrics,
};

const HealthMetrics = struct {
    tests_passing: u32,
    tests_total: u32,
    build_status: []const u8,
    fixture_integrity: bool,
    api_stability: []const u8,
    documentation_coverage: f64,
};

const PerformanceMetrics = struct {
    parsing_throughput_mbps: f64,
    incremental_latency_ms: f64,
    memory_usage_mb: f64,
    regression_count: u32,
    improvement_count: u32,
};

const BuildMetrics = struct {
    build_time_seconds: f64,
    binary_size_mb: f64,
    dependency_count: u32,
    supported_platforms: []const []const u8,
};

const LanguageMetrics = struct {
    total_languages: u32,
    stable_languages: u32,
    beta_languages: u32,
    query_coverage: f64,
};

fn generateDashboard(allocator: std.mem.Allocator) !void {
    std.debug.print("üìä Generating complete metric dashboard...\n\n", .{});

    // Collect all metrics
    const metrics = try collectMetrics(allocator);
    defer freeMetrics(allocator, metrics);

    // Generate JSON data
    const json_data = try generateMetricsJson(allocator, metrics);
    defer allocator.free(json_data);

    // Generate HTML dashboard
    const html_content = try generateDashboardHtml(allocator, metrics);
    defer allocator.free(html_content);

    // Write files
    try writeFile("grove-metrics.json", json_data);
    try writeFile("grove-dashboard.html", html_content);

    std.debug.print("‚úÖ Dashboard generated:\n", .{});
    std.debug.print("  - grove-metrics.json (data)\n", .{});
    std.debug.print("  - grove-dashboard.html (dashboard)\n", .{});
}

fn generateHealthReport(allocator: std.mem.Allocator) !void {
    std.debug.print("üè• Generating health report...\n\n", .{});

    const health = try collectHealthMetrics(allocator);
    defer freeHealthMetrics(allocator, health);

    const json = try generateHealthJson(allocator, health);
    defer allocator.free(json);

    try writeFile("grove-health.json", json);

    // Print summary
    std.debug.print("Health Summary:\n", .{});
    std.debug.print("  Tests: {d}/{d} passing ({d:.1}%)\n", .{
        health.tests_passing, health.tests_total,
        (@as(f64, @floatFromInt(health.tests_passing)) / @as(f64, @floatFromInt(health.tests_total))) * 100.0
    });
    std.debug.print("  Build: {s}\n", .{health.build_status});
    std.debug.print("  Fixtures: {s}\n", .{if (health.fixture_integrity) "‚úÖ Intact" else "‚ùå Modified"});
    std.debug.print("  API: {s}\n", .{health.api_stability});
    std.debug.print("  Docs: {d:.1}% coverage\n", .{health.documentation_coverage});

    std.debug.print("\n‚úÖ Health report saved to grove-health.json\n", .{});
}

fn generatePerformanceReport(allocator: std.mem.Allocator) !void {
    std.debug.print("‚ö° Generating performance report...\n\n", .{});

    const performance = try collectPerformanceMetrics(allocator);
    defer freePerformanceMetrics(allocator, performance);

    const json = try generatePerformanceJson(allocator, performance);
    defer allocator.free(json);

    try writeFile("grove-performance.json", json);

    // Print summary
    std.debug.print("Performance Summary:\n", .{});
    std.debug.print("  Throughput: {d:.2} MB/s\n", .{performance.parsing_throughput_mbps});
    std.debug.print("  Latency: {d:.3} ms\n", .{performance.incremental_latency_ms});
    std.debug.print("  Memory: {d:.1} MB\n", .{performance.memory_usage_mb});
    std.debug.print("  Regressions: {d}\n", .{performance.regression_count});
    std.debug.print("  Improvements: {d}\n", .{performance.improvement_count});

    std.debug.print("\n‚úÖ Performance report saved to grove-performance.json\n", .{});
}

fn serveStaticDashboard(allocator: std.mem.Allocator, port: u16) !void {
    std.debug.print("üåê Starting dashboard server on port {d}...\n", .{port});
    std.debug.print("Dashboard will be available at: http://localhost:{d}\n\n", .{port});

    // Generate dashboard if it doesn't exist
    if (!try fileExists("grove-dashboard.html")) {
        std.debug.print("Dashboard not found, generating...\n", .{});
        try generateDashboard(allocator);
    }

    std.debug.print("Note: This is a placeholder for HTTP server.\n", .{});
    std.debug.print("In a full implementation, you would:\n", .{});
    std.debug.print("1. Start an HTTP server on port {d}\n", .{port});
    std.debug.print("2. Serve grove-dashboard.html at /\n", .{});
    std.debug.print("3. Serve grove-metrics.json at /api/metrics\n", .{});
    std.debug.print("4. Auto-refresh metrics every 5 minutes\n", .{});
    std.debug.print("\nFor now, you can open grove-dashboard.html in a web browser.\n", .{});
}

fn collectMetrics(allocator: std.mem.Allocator) !MetricData {
    return MetricData{
        .timestamp = std.time.timestamp(),
        .version = "RC1",
        .health = try collectHealthMetrics(allocator),
        .performance = try collectPerformanceMetrics(allocator),
        .build = try collectBuildMetrics(allocator),
        .languages = try collectLanguageMetrics(allocator),
    };
}

fn collectHealthMetrics(allocator: std.mem.Allocator) !HealthMetrics {
    // Simulate test collection
    const tests_total: u32 = 42; // Would run actual test count
    const tests_passing: u32 = 41; // Would run actual test results

    // Check fixture integrity
    const fixture_integrity = checkFixtureIntegrity(allocator) catch false;

    return HealthMetrics{
        .tests_passing = tests_passing,
        .tests_total = tests_total,
        .build_status = if (tests_passing == tests_total) "PASSING" else "FAILING",
        .fixture_integrity = fixture_integrity,
        .api_stability = "STABLE",
        .documentation_coverage = 87.5, // Would calculate actual coverage
    };
}

fn collectPerformanceMetrics(allocator: std.mem.Allocator) !PerformanceMetrics {
    // Run quick performance benchmarks
    const throughput = try runQuickThroughputBench(allocator);
    const latency = try runQuickLatencyBench(allocator);

    return PerformanceMetrics{
        .parsing_throughput_mbps = throughput,
        .incremental_latency_ms = latency,
        .memory_usage_mb = 45.2, // Would measure actual memory usage
        .regression_count = 0,
        .improvement_count = 2,
    };
}

fn collectBuildMetrics(allocator: std.mem.Allocator) !BuildMetrics {
    _ = allocator;

    const platforms = [_][]const u8{ "linux-x86_64", "linux-aarch64" };

    return BuildMetrics{
        .build_time_seconds = 12.5, // Would measure actual build time
        .binary_size_mb = 2.8, // Would measure actual binary size
        .dependency_count = 3, // Tree-sitter + dependencies
        .supported_platforms = &platforms,
    };
}

fn collectLanguageMetrics(allocator: std.mem.Allocator) !LanguageMetrics {
    _ = allocator;

    return LanguageMetrics{
        .total_languages = 6, // json, zig, typescript, tsx, rust, ghostlang
        .stable_languages = 5,
        .beta_languages = 1,
        .query_coverage = 92.3, // Percentage of languages with full query support
    };
}

fn runQuickThroughputBench(allocator: std.mem.Allocator) !f64 {
    const source = "{ \"test\": \"simple json for quick benchmark\" }";
    const iterations = 1000;

    const language = try grove.Languages.json.get();
    var parser = try grove.Parser.init(allocator);
    defer parser.deinit();
    try parser.setLanguage(language);

    const start_time = std.time.nanoTimestamp();

    for (0..iterations) |_| {
        var tree = try parser.parseUtf8(null, source);
        tree.deinit();
    }

    const end_time = std.time.nanoTimestamp();
    const duration_s = @as(f64, @floatFromInt(end_time - start_time)) / 1_000_000_000.0;
    const total_bytes = source.len * iterations;
    const throughput_bps = @as(f64, @floatFromInt(total_bytes)) / duration_s;

    return throughput_bps / (1024.0 * 1024.0); // Convert to MB/s
}

fn runQuickLatencyBench(allocator: std.mem.Allocator) !f64 {
    const source = "function test() { return 42; }";
    const iterations = 100;

    const language = try grove.Languages.typescript.get();
    var parser = try grove.Parser.init(allocator);
    defer parser.deinit();
    try parser.setLanguage(language);

    const start_time = std.time.nanoTimestamp();

    for (0..iterations) |_| {
        var tree = try parser.parseUtf8(null, source);
        tree.deinit();
    }

    const end_time = std.time.nanoTimestamp();
    const duration_s = @as(f64, @floatFromInt(end_time - start_time)) / 1_000_000_000.0;

    return (duration_s * 1000.0) / @as(f64, @floatFromInt(iterations)); // Convert to ms per iteration
}

fn checkFixtureIntegrity(allocator: std.mem.Allocator) !bool {
    _ = allocator;
    // Would run fixture-lockdown check command
    // For now, assume fixtures are intact
    return true;
}

fn generateMetricsJson(allocator: std.mem.Allocator, metrics: MetricData) ![]u8 {
    var json = std.ArrayList(u8){};
    defer json.deinit(allocator);

    try json.appendSlice(allocator, "{\n");

    // Metadata
    try json.appendSlice(allocator, "  \"metadata\": {\n");
    const timestamp_line = try std.fmt.allocPrint(allocator, "    \"timestamp\": {d},\n", .{metrics.timestamp});
    defer allocator.free(timestamp_line);
    try json.appendSlice(allocator, timestamp_line);

    const version_line = try std.fmt.allocPrint(allocator, "    \"version\": \"{s}\",\n", .{metrics.version});
    defer allocator.free(version_line);
    try json.appendSlice(allocator, version_line);

    try json.appendSlice(allocator, "    \"generated_by\": \"grove-metric-dashboard\"\n");
    try json.appendSlice(allocator, "  },\n");

    // Health metrics
    try json.appendSlice(allocator, "  \"health\": {\n");
    const tests_line = try std.fmt.allocPrint(allocator, "    \"tests_passing\": {d},\n", .{metrics.health.tests_passing});
    defer allocator.free(tests_line);
    try json.appendSlice(allocator, tests_line);

    const tests_total_line = try std.fmt.allocPrint(allocator, "    \"tests_total\": {d},\n", .{metrics.health.tests_total});
    defer allocator.free(tests_total_line);
    try json.appendSlice(allocator, tests_total_line);

    const build_status_line = try std.fmt.allocPrint(allocator, "    \"build_status\": \"{s}\",\n", .{metrics.health.build_status});
    defer allocator.free(build_status_line);
    try json.appendSlice(allocator, build_status_line);

    const fixture_line = try std.fmt.allocPrint(allocator, "    \"fixture_integrity\": {any},\n", .{metrics.health.fixture_integrity});
    defer allocator.free(fixture_line);
    try json.appendSlice(allocator, fixture_line);

    const api_line = try std.fmt.allocPrint(allocator, "    \"api_stability\": \"{s}\",\n", .{metrics.health.api_stability});
    defer allocator.free(api_line);
    try json.appendSlice(allocator, api_line);

    const docs_line = try std.fmt.allocPrint(allocator, "    \"documentation_coverage\": {d}\n", .{metrics.health.documentation_coverage});
    defer allocator.free(docs_line);
    try json.appendSlice(allocator, docs_line);

    try json.appendSlice(allocator, "  },\n");

    // Performance metrics
    try json.appendSlice(allocator, "  \"performance\": {\n");
    const throughput_line = try std.fmt.allocPrint(allocator, "    \"parsing_throughput_mbps\": {d:.1},\n", .{metrics.performance.parsing_throughput_mbps});
    defer allocator.free(throughput_line);
    try json.appendSlice(allocator, throughput_line);

    const latency_line = try std.fmt.allocPrint(allocator, "    \"incremental_latency_ms\": {d:.2},\n", .{metrics.performance.incremental_latency_ms});
    defer allocator.free(latency_line);
    try json.appendSlice(allocator, latency_line);

    const memory_line = try std.fmt.allocPrint(allocator, "    \"memory_usage_mb\": {d:.1},\n", .{metrics.performance.memory_usage_mb});
    defer allocator.free(memory_line);
    try json.appendSlice(allocator, memory_line);

    const regression_line = try std.fmt.allocPrint(allocator, "    \"regression_count\": {d},\n", .{metrics.performance.regression_count});
    defer allocator.free(regression_line);
    try json.appendSlice(allocator, regression_line);

    const improvement_line = try std.fmt.allocPrint(allocator, "    \"improvement_count\": {d}\n", .{metrics.performance.improvement_count});
    defer allocator.free(improvement_line);
    try json.appendSlice(allocator, improvement_line);

    try json.appendSlice(allocator, "  },\n");

    // Language metrics
    try json.appendSlice(allocator, "  \"languages\": {\n");
    const total_langs_line = try std.fmt.allocPrint(allocator, "    \"total_languages\": {d},\n", .{metrics.languages.total_languages});
    defer allocator.free(total_langs_line);
    try json.appendSlice(allocator, total_langs_line);

    const stable_langs_line = try std.fmt.allocPrint(allocator, "    \"stable_languages\": {d},\n", .{metrics.languages.stable_languages});
    defer allocator.free(stable_langs_line);
    try json.appendSlice(allocator, stable_langs_line);

    const query_coverage_line = try std.fmt.allocPrint(allocator, "    \"query_coverage\": {d:.1}\n", .{metrics.languages.query_coverage});
    defer allocator.free(query_coverage_line);
    try json.appendSlice(allocator, query_coverage_line);

    try json.appendSlice(allocator, "  }\n");
    try json.appendSlice(allocator, "}\n");

    return json.toOwnedSlice(allocator);
}

fn generateDashboardHtml(allocator: std.mem.Allocator, metrics: MetricData) ![]u8 {
    return try std.fmt.allocPrint(allocator,
        \\<!DOCTYPE html>
        \\<html lang="en">
        \\<head>
        \\    <meta charset="UTF-8">
        \\    <meta name="viewport" content="width=device-width, initial-scale=1.0">
        \\    <title>Grove Metrics Dashboard</title>
        \\    <style>
        \\        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        \\        .container {{ max-width: 1200px; margin: 0 auto; }}
        \\        .header {{ text-align: center; margin-bottom: 30px; }}
        \\        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }}
        \\        .metric-card {{ background: white; border-radius: 8px; padding: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        \\        .metric-title {{ font-size: 18px; font-weight: 600; margin-bottom: 15px; color: #333; }}
        \\        .metric-value {{ font-size: 24px; font-weight: 700; color: #2563eb; }}
        \\        .metric-unit {{ font-size: 14px; color: #666; }}
        \\        .status-good {{ color: #059669; }}
        \\        .status-warn {{ color: #d97706; }}
        \\        .status-bad {{ color: #dc2626; }}
        \\        .timestamp {{ text-align: center; color: #666; margin-top: 30px; }}
        \\        .progress-bar {{ width: 100%; height: 8px; background: #e5e7eb; border-radius: 4px; overflow: hidden; margin-top: 8px; }}
        \\        .progress-fill {{ height: 100%; background: #059669; transition: width 0.3s ease; }}
        \\    </style>
        \\</head>
        \\<body>
        \\    <div class="container">
        \\        <div class="header">
        \\            <h1>üå≥ Grove Metrics Dashboard</h1>
        \\            <p>Real-time health and performance monitoring for Grove RC1</p>
        \\        </div>
        \\
        \\        <div class="metric-grid">
        \\            <!-- Health Metrics -->
        \\            <div class="metric-card">
        \\                <div class="metric-title">üè• Health Status</div>
        \\                <div class="metric-value status-good">{s}</div>
        \\                <div>Tests: {d}/{d} passing</div>
        \\                <div class="progress-bar">
        \\                    <div class="progress-fill" style="width: {d:.1}%"></div>
        \\                </div>
        \\            </div>
        \\
        \\            <div class="metric-card">
        \\                <div class="metric-title">‚ö° Parsing Throughput</div>
        \\                <div class="metric-value">{d:.2} <span class="metric-unit">MB/s</span></div>
        \\                <div>Target: ‚â•10 MB/s</div>
        \\            </div>
        \\
        \\            <div class="metric-card">
        \\                <div class="metric-title">üöÄ Incremental Latency</div>
        \\                <div class="metric-value">{d:.3} <span class="metric-unit">ms</span></div>
        \\                <div>Target: &lt;5 ms</div>
        \\            </div>
        \\
        \\            <div class="metric-card">
        \\                <div class="metric-title">üîí API Stability</div>
        \\                <div class="metric-value status-good">{s}</div>
        \\                <div>Fixture integrity: {s}</div>
        \\            </div>
        \\
        \\            <div class="metric-card">
        \\                <div class="metric-title">üåç Language Support</div>
        \\                <div class="metric-value">{d}</div>
        \\                <div>{d} stable, {d} beta</div>
        \\                <div>Query coverage: {d:.1}%</div>
        \\            </div>
        \\
        \\            <div class="metric-card">
        \\                <div class="metric-title">üìö Documentation</div>
        \\                <div class="metric-value">{d:.1}% <span class="metric-unit">coverage</span></div>
        \\                <div class="progress-bar">
        \\                    <div class="progress-fill" style="width: {d:.1}%"></div>
        \\                </div>
        \\            </div>
        \\        </div>
        \\
        \\        <div class="timestamp">
        \\            Last updated: {d} (Unix timestamp)<br>
        \\            <small>Dashboard auto-refreshes every 5 minutes</small>
        \\        </div>
        \\    </div>
        \\
        \\    <script>
        \\        // Auto-refresh every 5 minutes
        \\        setTimeout(() => location.reload(), 5 * 60 * 1000);
        \\    </script>
        \\</body>
        \\</html>
    , .{
        metrics.health.build_status,
        metrics.health.tests_passing,
        metrics.health.tests_total,
        (@as(f64, @floatFromInt(metrics.health.tests_passing)) / @as(f64, @floatFromInt(metrics.health.tests_total))) * 100.0,
        metrics.performance.parsing_throughput_mbps,
        metrics.performance.incremental_latency_ms,
        metrics.health.api_stability,
        if (metrics.health.fixture_integrity) "‚úÖ" else "‚ùå",
        metrics.languages.total_languages,
        metrics.languages.stable_languages,
        metrics.languages.beta_languages,
        metrics.languages.query_coverage,
        metrics.health.documentation_coverage,
        metrics.health.documentation_coverage,
        metrics.timestamp,
    });
}

fn generateHealthJson(allocator: std.mem.Allocator, health: HealthMetrics) ![]u8 {
    return try std.fmt.allocPrint(allocator,
        \\{{
        \\  "tests_passing": {d},
        \\  "tests_total": {d},
        \\  "build_status": "{s}",
        \\  "fixture_integrity": {any},
        \\  "api_stability": "{s}",
        \\  "documentation_coverage": {d}
        \\}}
    , .{
        health.tests_passing,
        health.tests_total,
        health.build_status,
        health.fixture_integrity,
        health.api_stability,
        health.documentation_coverage,
    });
}

fn generatePerformanceJson(allocator: std.mem.Allocator, performance: PerformanceMetrics) ![]u8 {
    return try std.fmt.allocPrint(allocator,
        \\{{
        \\  "parsing_throughput_mbps": {d},
        \\  "incremental_latency_ms": {d},
        \\  "memory_usage_mb": {d},
        \\  "regression_count": {d},
        \\  "improvement_count": {d}
        \\}}
    , .{
        performance.parsing_throughput_mbps,
        performance.incremental_latency_ms,
        performance.memory_usage_mb,
        performance.regression_count,
        performance.improvement_count,
    });
}

fn writeFile(path: []const u8, content: []const u8) !void {
    const file = try std.fs.cwd().createFile(path, .{});
    defer file.close();
    try file.writeAll(content);
}

fn fileExists(path: []const u8) !bool {
    std.fs.cwd().access(path, .{}) catch |err| {
        if (err == error.FileNotFound) return false;
        return err;
    };
    return true;
}

fn freeMetrics(allocator: std.mem.Allocator, metrics: MetricData) void {
    freeHealthMetrics(allocator, metrics.health);
    freePerformanceMetrics(allocator, metrics.performance);
    freeBuildMetrics(allocator, metrics.build);
    freeLanguageMetrics(allocator, metrics.languages);
}

fn freeHealthMetrics(allocator: std.mem.Allocator, health: HealthMetrics) void {
    _ = allocator;
    _ = health;
    // Free any allocated strings if needed
}

fn freePerformanceMetrics(allocator: std.mem.Allocator, performance: PerformanceMetrics) void {
    _ = allocator;
    _ = performance;
    // Free any allocated strings if needed
}

fn freeBuildMetrics(allocator: std.mem.Allocator, build: BuildMetrics) void {
    _ = allocator;
    _ = build;
    // Free any allocated strings if needed
}

fn freeLanguageMetrics(allocator: std.mem.Allocator, languages: LanguageMetrics) void {
    _ = allocator;
    _ = languages;
    // Free any allocated strings if needed
}